{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["jCoOSFyymw4_"],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyP725EZ8ZPilP8tH060FJl8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 0. Set up environment"],"metadata":{"id":"z1klwSvflPJv"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RdNpYZmqlOFh","executionInfo":{"status":"ok","timestamp":1696493276404,"user_tz":-420,"elapsed":6908,"user":{"displayName":"Hoàng Trần","userId":"03707623474819194049"}},"outputId":"dd904ce8-a9bb-4aeb-c41a-0afe5d97f2ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting open_clip_torch\n","  Downloading open_clip_torch-2.20.0-py3-none-any.whl (1.5 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (0.15.2+cu118)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (2023.6.3)\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (6.1.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (4.66.1)\n","Collecting huggingface-hub (from open_clip_torch)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece (from open_clip_torch)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf<4 in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (3.20.3)\n","Collecting timm (from open_clip_torch)\n","  Downloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->open_clip_torch) (3.27.5)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->open_clip_torch) (17.0.1)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->open_clip_torch) (0.2.7)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch) (6.0.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch) (23.1)\n","Collecting safetensors (from timm->open_clip_torch)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->open_clip_torch) (1.23.5)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->open_clip_torch) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.0.5)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\n","Installing collected packages: sentencepiece, safetensors, huggingface-hub, timm, open_clip_torch\n","Successfully installed huggingface-hub-0.17.3 open_clip_torch-2.20.0 safetensors-0.3.3 sentencepiece-0.1.99 timm-0.9.7\n"]}],"source":["!pip install open_clip_torch"]},{"cell_type":"code","source":["import os\n","import cv2\n","import json\n","import shutil\n","import copy\n","import zipfile\n","import numpy as np\n","from tqdm import tqdm\n","import torch\n","import torchvision\n","from torch import nn\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import multiprocessing\n","from multiprocessing.pool import Pool\n","import glob\n","import math\n","from PIL import Image\n","import open_clip"],"metadata":{"id":"rHIVaygglkJW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"],"metadata":{"id":"EFVNL_S_lrCh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l6dmbgTAlsHF","executionInfo":{"status":"ok","timestamp":1696498215214,"user_tz":-420,"elapsed":4134,"user":{"displayName":"Hoàng Trần","userId":"03707623474819194049"}},"outputId":"d81c84a3-a0f5-436f-c559-5b6c9ea56fa4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Setup device agnostic code\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"EcjE0iScltQx","executionInfo":{"status":"ok","timestamp":1696498215214,"user_tz":-420,"elapsed":7,"user":{"displayName":"Hoàng Trần","userId":"03707623474819194049"}},"outputId":"609ec60c-8734-4e90-94ff-b263abec80fb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["DATA_PATH = 'data'\n","SAVE_PATH = 'result'\n","DRIVE_PATH = 'drive/MyDrive/AI_Challenge'\n","if not os.path.exists(DATA_PATH):\n","    os.mkdir(DATA_PATH)\n","if not os.path.exists(SAVE_PATH):\n","    os.mkdir(SAVE_PATH)"],"metadata":{"id":"dTaSyCYkluNB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Load data"],"metadata":{"id":"FGm8ZuwJlwgH"}},{"cell_type":"code","source":["VID_TO_HANDLE = (1, 3, 4)"],"metadata":{"id":"0HTIs4D6lxCv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_data(path, save_path):\n","    with zipfile.ZipFile(path, 'r') as zip_ref:\n","        zip_ref.extractall(save_path)\n","    os.remove(path)\n","\n","# Copy videos from drive and extract\n","def download_vid(i):\n","    # p = None\n","    vid_path = f'Videos_L0{i}.zip' if i < 10 else f'Videos_L{i}.zip'\n","    if os.path.exists(vid_path):\n","        os.remove(vid_path)\n","    shutil.copy(os.path.join(DRIVE_PATH, 'TransNetV2_result', vid_path), '.')\n","\n","    save_path = os.path.join(DATA_PATH, vid_path.split('.')[0])\n","    if os.path.exists(save_path):\n","        shutil.rmtree(save_path)\n","    os.mkdir(save_path)\n","\n","    with zipfile.ZipFile(vid_path, 'r') as zip_ref:\n","        zip_ref.extractall(save_path)\n","    os.remove(vid_path)\n","\n","    #     if p is not None:\n","    #         p.join()\n","    #     p = multiprocessing.Process(target=extract_data, args=(vid_path, save_path))\n","    #     p.start()\n","    # if p is not None:\n","    #     p.join()"],"metadata":{"id":"RX9El2fglylW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Dataloader"],"metadata":{"id":"jCoOSFyymw4_"}},{"cell_type":"code","source":["class CLIPDataset(Dataset):\n","    def __init__(\n","        self,\n","        img_paths,\n","        transform,\n","    ):\n","        self.img_paths = img_paths\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        img = self.transform(Image.open(img_path))\n","        return img"],"metadata":{"id":"SXv75zKemzBL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. CLIP image encoder"],"metadata":{"id":"yQShYqBel-n8"}},{"cell_type":"code","source":["def zip2drive(vid_pack):\n","    shutil.rmtree(os.path.join(DATA_PATH, vid_pack))\n","    res_path = os.path.join(SAVE_PATH, vid_pack)\n","\n","    shutil.make_archive(res_path, 'zip', res_path)\n","    shutil.rmtree(res_path)\n","\n","    save_dir = os.path.join(DRIVE_PATH, 'clip_vitl14_result')\n","    if not os.path.exists(save_dir):\n","        os.mkdir(save_dir)\n","\n","    shutil.move(res_path + '.zip', save_dir)"],"metadata":{"id":"8Scdr4qFqypB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='datacomp_xl_s13b_b90k')\n","model.to(device).eval()"],"metadata":{"id":"4ks5YuuY5DOl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 512\n","is_first = True\n","downloading = None\n","for vid_id in VID_TO_HANDLE:\n","    downloading = multiprocessing.Process(target=download_vid, args=(vid_id, ))\n","    downloading.start()\n","\n","    if not is_first:\n","        move2drive = []\n","        for video_pack in os.listdir(DATA_PATH):\n","            full_video_pack = os.path.join(DATA_PATH, video_pack)\n","            for video in os.listdir(full_video_pack):\n","                # load dataset\n","                clip_dataset = CLIPDataset(\n","                    img_paths = sorted(glob.glob(os.path.join(full_video_pack, video, 'frames/*.png'))),\n","                    transform = preprocess\n","                )\n","                clip_dataloader = DataLoader(\n","                    clip_dataset,\n","                    batch_size=batch_size,\n","                    shuffle=False,\n","                    drop_last=False,\n","                    pin_memory=True,\n","                    num_workers=4,\n","                )\n","                save_path = os.path.join(SAVE_PATH, video_pack)\n","                if not os.path.exists(save_path):\n","                    os.mkdir(save_path)\n","                save_path = os.path.join(save_path, video)\n","\n","                results = []\n","                with torch.inference_mode():\n","                    for batch, img in enumerate(tqdm(clip_dataloader, desc=f\"{os.path.join(video_pack, video)}: \")):\n","                        results.append(model.encode_image(img.to(device)).float().cpu().numpy())\n","\n","                results = np.concatenate(results, axis = 0)\n","\n","                np.save(f'{save_path}.npy', results)\n","\n","            p = multiprocessing.Process(target=zip2drive, args=(video_pack, ))\n","            p.start()\n","            move2drive.append(p)\n","\n","        for p in move2drive:\n","            p.join()\n","\n","    else:\n","        is_first = False\n","    downloading.join()\n",""],"metadata":{"id":"5C7AkJCCmRBM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"n6nEyNVEwMZQ"},"execution_count":null,"outputs":[]}]}